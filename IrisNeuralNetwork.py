# -*- coding: utf-8 -*-
"""NeuralNetworkIris2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lxj2rjg9yNHvZOryc0Sa0Lo5654ddt6z
"""

import os
import torch 
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np



from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader


# import dataset
import seaborn as sns

iris = sns.load_dataset('iris')

#train_ds, test_ds = sns.load_dataset('iris')

#train_ds, test_ds = get_dataset(sns.load_dataset('iris'))

iris.head()

iris['species'] = iris['species'].astype('category')
iris['species'] = iris['species'].cat.codes
iris.head()

n = len(iris.index)
print(n)
shuffle_indices = np.random.permutation(n)
iris = iris.iloc[shuffle_indices]
iris.head()

x = iris.iloc[:, :4].values.astype(np.float32)
y = iris.iloc[:, -2].values.astype(np.int64)

mu = x.mean(axis=0)
span = x.max(axis=0) - x.min(axis=0)
print(mu, span)

def rescale(inputs):
  return (inputs - mu) / span

x = rescale(x)
print(x[:5])

num_train = int(n * .6)
num_test = n - num_train

x_train = x[:num_train]
y_train = y[:num_train]
x_test = x[-num_test:]
y_test = y[-num_test:]

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

class NpDataset(Dataset):

  def __init__(self, data, label):

    assert len(data) == len(label)

    self.data = torch.from_numpy(data)
    self.label = torch.from_numpy(label)

  def __getitem__(self, index):

    return self.data[index], self.label[index]

  def __len__(self):

    return len(self.label)

train_dataset = NpDataset(x_train, y_train)
test_dataset = NpDataset(x_test, y_test)

train_dataloader = DataLoader(
    
    train_dataset,
    batch_size = 128,
    shuffle=False

)

test_dataloader = DataLoader(
    
    test_dataset,
    batch_size = 128,
    shuffle=False

)

print(len(train_dataloader.dataset), len(test_dataloader.dataset))

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

class IrisNN(nn.Module):

  def __init__(self):

    super(IrisNN, self).__init__()

    self.fn1 = nn.Linear(4, 6)
    self.fn2 = nn.Linear(6, 3)

  def forward(self, x):

    x = F.relu(self.fn1(x))
    x = self.fn2(x)
    return x

model = IrisNN()
model.to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay=0.01)

x, y = next(iter(train_dataloader))
x = x[:5].to(device)
score = model(x)
print(score)

def train():

  model.train()
  for x, y in train_dataloader:
    x = x.to(device)
    y = y.to(device)
    n = x.size(0)

    optimizer.zero_grad()
    score = model(x)
    loss = loss_fn(score, y)

    loss.backward()
    optimizer.step()

    predictions = score.max(1, keepdim = True)[1]
    num_correct = predictions.eq(y.view_as(predictions)).sum().item()

  acc = num_correct / n
  return loss, acc

def evaluate():
  model.eval()
  with torch.no_grad():
    for x, y in test_dataloader:
      x = x.to(device)
      y = y.to(device)
      n = x.size(0)

      score = model(x)
      loss = loss_fn(score, y)

      predictions = score.max(1, keepdim = True)[1]
      num_correct = predictions.eq(y.view_as(predictions)).sum().item()

  acc = num_correct / n
  return loss, acc

max_epochs = 100
for epoch in range(max_epochs):
  tr_loss, tr_acc = train()
  eva_loss, eva_acc = evaluate()

  print('[{}/{}] Train loss:{:.4f} acc:{:.2f}% - Test loss:{:4f} acc:{:.2f}'.format(epoch, max_epochs, tr_loss, tr_acc*100, eva_loss, eva_acc*100))

  # Loss is decreasing in this cell's output, which is a good sign

# Expected initial loss

np.log(3)

# Beginning: [0/100] Train loss:1.0430 acc:47.78% - Test loss:1.039049 acc:46.67
# End of 1st Iteration: [99/100] Train loss:0.2523 acc:93.33% - Test loss:0.229903 acc:93.33
# End of 2nd Iteration: [99/100] Train loss:0.1551 acc:97.78% - Test loss:0.145689 acc:95.00

# [99/100] Train loss:0.1445 acc:97.78% - Test loss:0.136950 acc:95.00
# [99/100] Train loss:0.1429 acc:97.78% - Test loss:0.135467 acc:96.67
# [99/100] Train loss:0.1416 acc:97.78% - Test loss:0.134619 acc:96.67
# [99/100] Train loss:0.1407 acc:97.78% - Test loss:0.133764 acc:96.67
# [99/100] Train loss:0.1401 acc:97.78% - Test loss:0.133499 acc:96.67
# [99/100] Train loss:0.1399 acc:97.78% - Test loss:0.133417 acc:96.67

"""THINGS TO LEARN
<br>
<br>
<br>

Loss function for classification with label encoding

1.   log_softmax(x) and NLLLoss (Negative Log Likelihood loss)
2.   softmax(x) does NOT work with NLLLoss
3.   CrossEntropyLoss combines these 2 together

Optimization

1.   SGD (Stochastic gradient descent)
2.   Adam 
3.   weight_decay: add L2 penalty




"""