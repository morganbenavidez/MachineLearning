# -*- coding: utf-8 -*-
"""LinearRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JaHLRKczJoUN-cymnkiJW4fLLvWVu-kX
"""

# $ Linear Regression
# Take continuous data and figure out a best fit line to that data
# Features and Labels - Important in Supervised Learning
# Quandl gives you lots of data sets - limited calls per day without an account


!pip install sklearn
!pip install quandl
!pip install pandas

import pandas as pd
import quandl
import math, datetime
import numpy as np
import sklearn

# preprocessing helps us scale data on features, get them between -1 and 1
# may choose to not use preprocessing
# cross validation used to create our training and testing samples, shuffles data to reduce bias
# svm support vector machine
#from sklearn import preprocessing, cross_validate, svm

from sklearn import preprocessing, svm

from sklearn.linear_model import LinearRegression

from sklearn.model_selection import cross_validate

from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
from matplotlib import style
import pickle

style.use('ggplot')

df = quandl.get('WIKI/GOOGL')
print(df.head())

# Open, High, Low, Close, Volume are all features

# Can discover relationships between attributes in deep learning, but not with regression
# You want to simplify your data as much as possible, useless data can cause problems with your classification

df = df[['Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close', 'Adj. Volume']]

# Open tells us the opening price of a stock for the day
# The relationship between High and Low, tells us the volatility for the day
# Close tells us if it went up or down from the Open Price
# Volume is the number of trades made that day

# $ Percent Volatility

# The dividend is subtracting low from high, row by row
df['HighLowPercent'] = (df['Adj. High'] - df['Adj. Close']) / df['Adj. Close'] * 100.0
df['PercentChange'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.0

# Below are our features
df = df[['Adj. Close', 'HighLowPercent', 'PercentChange', 'Adj. Volume']]

print(df.head)

ForecastColumn = 'Adj. Close'

# df.fillna in pandas will replace empty data with NAN.
# In Machine Learning, we can't work with NAN, so we input a value (-9999) instead of NAN
# Inputing this number will treat this data entry as an outlier
df.fillna(-9999, inplace=True)

# ForecastOut will be the number of days out we're predicting
# 0.1 will predict out 10% of the dataframe
ForecastOut = int(math.ceil(0.01*len(df)))
print('Total Days in Dataset: ' + str(len(df)))
print('ForecastOut: ' + str(ForecastOut))

df['label'] = df[ForecastColumn].shift(-ForecastOut)
#df.dropna(inplace=True)

print(df.head())  # Shows First 5 Entries in Data Set
print(df.tail())  # Shows Last 5 Entries in Data Set


# Features usually uppercase X
# df.drop is returning a new dataframe and then being converted to a numpy array
X = np.array(df.drop(['label'], 1))
X = preprocessing.scale(X)
X = X[:-ForecastOut]
X_lately = X[-ForecastOut:] # Stuff we're going to predict against

#X = X[:-ForecastOut+1]
df.dropna(inplace=True)

# Labels usually a lowercase y
y = np.array(df['label'])

print(len(X), len(y)) # These numbers should be the same

# 0.2 is 20% of our data will be used to train our model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Define our classifier
# You can change the algorithm you want to use here by changing what you assign to clf
#clf = svm.SVR(kernel='poly') # This one does much worse on accuracy
clf = LinearRegression()
# You can add n_jobs as an argument to LinearRegression.
# i.e LinearRegression(n_jobs=10)
# LinearRegression(n_jobs=-1) -> This (-1) will have your computer run as many threads as possible 
# by your Processor.  This speeds up training time

# To fit (or train) or train our classifier, fit is synonymous with train
clf.fit(X_train, y_train)

# This will store the pickle 
with open('linearregression.pickle', 'wb') as f:

  pickle.dump(clf, f)

# This will load the pickle
pickle_in = open('linearregression.pickle', 'rb')
clf = pickle.load(pickle_in)

# Score is synonymous with test
accuracy = clf.score(X_test, y_test)

print('accuracy: ' + str(accuracy))


# Start 5


ForecastSet = clf.predict(X_lately)

print(ForecastSet, accuracy, ForecastOut)

df['Forecast'] = np.nan

last_date = df.iloc[-1].name
last_unix = last_date.timestamp()
one_day = 86400
next_unix = last_unix + one_day

for i in ForecastSet:

  next_date = datetime.datetime.fromtimestamp(next_unix)
  next_unix += one_day
  df.loc[next_date] = [np.nan for _ in range(len(df.columns)-1)] + [i]


print(df.head())
print(df.tail())

df['Adj. Close'].plot()
df['Forecast'].plot()
plt.legend(loc=4)
plt.xlabel('Date')
plt.ylabel('Price')
plt.show()


# Pickling - will let you save your Classifier and then just quickly load it in without any training time
# Use your trained model to predict things, without having to train every single time



# Start 6


# Pickling is the serialization of any Python object, so this could be a 
# dictionary or a classifier or any other object
# Pickle (or save) the classifier after training to avoid having to run through the training process again
# This is great for very large datasets because this would become very costly
# You may want to retrain once a month or something like that



# Start 7 



# Breakdown Linear Regression

